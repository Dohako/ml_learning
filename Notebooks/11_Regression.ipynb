{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pr1_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP7tStEZp_8v"
      },
      "source": [
        "# Импорт необходимых модулей \n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Настройки для визуализации\n",
        "# Если используется темная тема - лучше текст сделать белым\n",
        "TEXT_COLOR = 'black'\n",
        "\n",
        "matplotlib.rcParams['figure.figsize'] = (15, 10)\n",
        "matplotlib.rcParams['text.color'] = 'black'\n",
        "matplotlib.rcParams['font.size'] = 14\n",
        "matplotlib.rcParams['axes.labelcolor'] = TEXT_COLOR\n",
        "matplotlib.rcParams['xtick.color'] = TEXT_COLOR\n",
        "matplotlib.rcParams['ytick.color'] = TEXT_COLOR\n",
        "\n",
        "# Зафиксируем состояние случайных чисел\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt3KM7DHYtmJ"
      },
      "source": [
        "# Линейная регрессия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzg51F2ZaXLm"
      },
      "source": [
        "Это наша первая практика, в которой мы коснемся вопросов, связанных с машинным обучением. Запомните, в машинном обучении **данные** - всему голова! Поэтому создадим немного данных, чтобы на них разбираться."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwH7rsLLqcFo",
        "outputId": "f3a8cc2d-50a2-487b-f3ae-83247eed692c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "n_points = 100\n",
        "\n",
        "real_W = [1, 0.7]\n",
        "X_data = 4*np.sort(np.random.rand(n_points, 1), axis=0)+1\n",
        "noize = 1*(np.random.rand(n_points, 1)-0.5)\n",
        "y_data_true = real_W[0] + real_W[1]*X_data\n",
        "y_data_noized = y_data_true + noize\n",
        "y_data = y_data_noized[:, 0]\n",
        "\n",
        "X_render = np.linspace(X_data[:, 0].min(), X_data[:, 0].max(), 100)\n",
        "y_render = real_W[0] + real_W[1]*X_render\n",
        "\n",
        "plt.scatter(X_data, y_data_noized, label='Данные')\n",
        "plt.ylabel('$y$')\n",
        "plt.xlabel('$x$')\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd69c616ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEAU9dr88y_h"
      },
      "source": [
        "Отлично! Вот мы имеет данные, но что это и как с этим работать? Эти данные нами сгенерированы, но в реальных данных всему есть назначение, поэтому и мы зададимся некоторым описанием:\n",
        "- $x$ - качество продукта [безразмерный показатель];\n",
        "- $y$ - цена продукта [у.е.].\n",
        "\n",
        "В виде таблицы это бы выглядело вот так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhJSF-Gq_lCD",
        "outputId": "27087499-4ebc-4819-9163-13d2d50cbd8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "pd.DataFrame({\n",
        "    'quality': X_data[:,0],\n",
        "    'price': y_data\n",
        "}).head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>quality</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.022088</td>\n",
              "      <td>1.246891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.082338</td>\n",
              "      <td>1.894047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.101677</td>\n",
              "      <td>1.585530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.137554</td>\n",
              "      <td>1.804859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.180909</td>\n",
              "      <td>2.234203</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    quality     price\n",
              "0  1.022088  1.246891\n",
              "1  1.082338  1.894047\n",
              "2  1.101677  1.585530\n",
              "3  1.137554  1.804859\n",
              "4  1.180909  2.234203"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9Kpr9w3_hCF"
      },
      "source": [
        "Теперь данные имеют смысл. Каждая точка на графике (запись в таблице) - отдельный продукт из базы и мы видим, что с увеличением качества продукта растет и цена - это и есть **зависимость** в данных. Зависимость переменной $y$ от переменной $x$, а значит можем назвать эти переменные так:\n",
        "- $y$ - зависимая переменная;\n",
        "- $x$ - независимая переменная.\n",
        "\n",
        "Еще мы можем видеть, что в данных бывают случаи, когда качество ниже, но цена выше! Что это такое? Это статистика! \n",
        "\n",
        "Если рассмотреть две отдельные точки, то можно попасть на такие, что качество выше, но цена ниже. Тогда мы делаем вывод (по двум точкам), что выше качество - ниже цена. **Плохо!**\n",
        "\n",
        "Статистика по большому количеству данных позволяет смотреть не на отдельные точки, а на зависимость в общем. Такой взгляд показывает, как дела обстоят на самом деле, а не в случае двух отдельных продуктов.\n",
        "\n",
        "Немного формализуя, можно сказать, что данные представляют собой следующий вид:\n",
        "$$\n",
        "данные = зависимости + шум\n",
        "$$\n",
        "\n",
        "Здесь зависимость - это то, что ценно и полезно для тех, кто хочет использовать данные (во благо или во зло уже другой вопрос), а шум - отклонения, разброс, выбросы и другие девиации в данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNCq_-zqBtXP"
      },
      "source": [
        "Зависимости бывают разные и спасибо математике за возможность описать не только словами, но и даже формулами (аналитически). Логарифмическая, полиномиальная, экспоненциальная - много разных, но мы смотрим на нашу.\n",
        "\n",
        "Что если попробуем провести две прямых линии через эти данные? Пробуем!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0k0041gCPMy",
        "outputId": "e7cff7e5-bc60-451f-82a6-4ea5de3b23b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "X_render = np.linspace(X_data[:, 0].min(), X_data[:, 0].max(), 100)\n",
        "print(f'real_W is {real_W}')\n",
        "y1 = real_W[0] + real_W[1]*X_render\n",
        "y2 = 0.7 + 0.1*X_render\n",
        "plt.scatter(X_data, y_data_noized, label='Данные')\n",
        "plt.plot(X_render, y1, 'k-', label='Прямая линия 1')\n",
        "plt.plot(X_render, y2, 'k-.', label='Прямая линия 2')\n",
        "plt.ylabel('$y$')\n",
        "plt.xlabel('$x$')\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "real_W is [1, 0.7]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd69c027780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYIwvecjCYhx"
      },
      "source": [
        "Хм, разбираемся:\n",
        "- Линия 1 - проходит через данные так, что общая зависимость в данных более менее похожа на нее;\n",
        "- Линия 2 - очень сильно смещена от данных и имеет другой рост значения (угол наклона), нежели данные.\n",
        "\n",
        "Делаем вывод, что линия 1 лучше **описывает** данные, чем линия 2. В чем разница между линиями? У каждой свои коэффициенты $k$ и $b$.\n",
        "\n",
        "> Если кто запамятовал уравнения прямой линии: $y=kx+b$\n",
        "\n",
        "То есть, мы сейчас визуально убедились, что зависимость в данных имеет линейный характер, так как по прямой линии 1 рост идет таким же темпом, а отклонения данных от линии сохраняются более менее равномерно.\n",
        "\n",
        "Если зависимость в данных имеет линейный характер, значит и описать ее можно уравнением прямой линии. Но это должна быть не просто любая прямая линия (как уже убедились), а с такими коэффициентами, при которых линия будет идти в соответсвии с зависимостью в данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akFW7uZRFt-9"
      },
      "source": [
        "Так мы подошли к понятию **задачи определения регрессии**.\n",
        "\n",
        "> **Задача определения регрессии** - определение аналитического описания (формулы) зависимости зависимой переменной $y$ от независимых переменных $x$ (да, может быть несколько переменных, например, качество и срок годности).\n",
        "\n",
        "В машинном обучении, решая задачу определения регрессии, мы ищем такую функцию, которая максимально точно опишет зависимость в данных.\n",
        "\n",
        "В нашем случае данные имеют линейную зависимость. Поэтому мы будем использовать **модель линейной регрессии**.\n",
        "\n",
        "Что такое **модель**? В машинном обучении модель - более общее понятие функции/алгоритма. Линейная зависимость определяется уравнением прямой, экспоненциальная - уравнением экспоненты, но есть более сложные зависимости, которые описываются не уравнением, а более сложными вещами (типа решающих деревьев).\n",
        "\n",
        "Так вот, модель линейной регрессии для нашего случая похожа на уравнение прямой:\n",
        "$$\n",
        "\\hat{y} = b + k*x\n",
        "$$\n",
        "\n",
        "Модель имеет **параметры**, которые и характеризуют ее поведение. В случае линейной регрессии это и есть наши $k$ и $b$. Как мы видели, при одних параметрах модель \"хорошо\" описывает данные, а при других - \"плохо\". \n",
        "\n",
        "В уравнении $\\hat{y}$ - **предсказание** модели. Данные, которые мы имеет $x$ и $y$ используются для **обучения**, а использование модели - **предсказать** цену на основе новых неизвестных ранее $x$ (новая оценка качества). Так как задача модели - отражать зависимость в данных, то предсказания должны также соответствовать общей зависимости.\n",
        "\n",
        "Таким образом, после выбора модели мы должны произвести **обучение**. Если кратко, **обучение** - процесс изменения параметров (их еще называют весами модели) так, чтобы модель работала \"хорошо\" (в задаче регрессии - \"хорошо\" описывала закономерности в данных).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-Vcp8OtNA9h"
      },
      "source": [
        "Резюмируя, имеем данные по продуктам и хотим решить задачу регрессии, видим линейную зависимость в данных, обучаем модель линейной регрессии и она уже по новым данным оценок качества сама предсказывает и устанавливает цену на продукты! Прекрасная автоматизация!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWrotA3qNzQe"
      },
      "source": [
        "Если все понятно, то немного копнем поглубже. Обратите внимание, что параметры в нашей модели - это то, насколько качество влияет на цену ($k$) и смещение средней цены ($b$). Величину влияния качества на цену можно назвать весом качества, так как чем больше $k$, тем больше результарующая цена. \n",
        "\n",
        "Смещение нужно, чтобы подкорректировать работу модели. В наших данных нет записей при $x=0$, но если посмотреть общую тенденцию, то при $x=0$, $y \\neq 0$. Да и в уравнении прямой всегда было смещение, ведь иначе все линии проходили бы через точку $(0, 0)$ - центр координат.\n",
        "\n",
        "Так как $k$ мы обозвали весом, то правильнее переопределить переменную. Ведь это не просто уравнение прямой, а целая модель линейной регрессии! Назовем это так:\n",
        "$$\n",
        "\\hat{y} = w_0 + w*x\n",
        "$$\n",
        "\n",
        "где $w_0$ - константное смещение, $w$ - вес переменной $x$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rx1SC6cyvSl"
      },
      "source": [
        "# Функция предсказания"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-7krPgTaXLn"
      },
      "source": [
        "Для нашего случая, когда мы имеем одну независимую переменную (признак) в данных:\n",
        "\n",
        "$$\n",
        "\\hat{y} = w_0 + w*x\n",
        "$$\n",
        "\n",
        "Теперь распространим на случай, когда в данных переменная $y$ зависит от двух принаков: $x_1$ (качество продукта) и $x_2$ (срок годности):\n",
        "\n",
        "$$\n",
        "\\hat{y} = w_0 + w_1*x_1 + w_2*x_2\n",
        "$$\n",
        "\n",
        "Как видно, модель линейной регрессии каждому признаку присваивает вес, поэтому можно записать общий вид модели:\n",
        "\n",
        "$$\n",
        "\\hat{y} = h_W(x)= w_0 + \\sum_{i=1}^{m}w_i*x_i\n",
        "$$\n",
        "\n",
        "> $h_W(x)$ - функция предсказания модели, которая имеет веса для каждого признака\n",
        "\n",
        "где $m$ - кол-во признаков в данных, $w_i$ - вес признака $x_i$, $w_0$ - константное смещение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2afAH0BuSWDR"
      },
      "source": [
        "> Еще раз для закрепления, модель линейной регрессии - линейное уравнение, в котором каждый признак данных имеет свой вес. В ходе обучения на данных настраиваются веса каждого признака и смещение модели, чтобы модель повторяла зависимость в данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx7FGSlBbGfq"
      },
      "source": [
        "Отлично, теперь мы можем перейти к векторному представлению, так как здесь явно проглядывается формула скалярного произведения!\n",
        "\n",
        "Зададим признаки и веса в виде векторов:\n",
        "$$\n",
        "X = \n",
        "\\begin{bmatrix}\n",
        "1 & x_1 & \\dots & x_{m-1} & x_m\n",
        "\\end{bmatrix} \\\\\n",
        "W = \n",
        "\\begin{bmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\vdots \\\\\n",
        "w_{m-1} \\\\\n",
        "w_m\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Теперь мы можем записать представление модели в векторной форме: \n",
        "$$\n",
        "h_W(X)=XW\n",
        "$$\n",
        "\n",
        "> Здесь $h_W(X)$ означает функцию предсказания модели, в ней фигурируют веса $W$ и входные данные $X$. Результатом этой функции является предсказание модели $\\hat{y}$.\n",
        "\n",
        "Такой вид и запомним, так как он больше вообще не изменится!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xXgsOPrcu7u"
      },
      "source": [
        "Мы рассмотрели случай, когда у нас есть одна запись в данных и несколько признаков, но модели машинного обучения должны строиться на основе множества данных, чтобы вычислять основные зависимости, поэтому обязательно необходимо понимать представление, когда имеются $n$ записей данных. Для такого случая, все, что нам надо исправить в представлении - вектор $X$ превратится в матрицу:\n",
        "\n",
        "$$\n",
        "X = \n",
        "\\begin{bmatrix}\n",
        "1 & x^{(1)}_1 & \\dots & x^{(1)}_{m-1} & x^{(1)}_m \\\\\n",
        "1 & x^{(2)}_1 & \\dots & x^{(2)}_{m-1} & x^{(2)}_m \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots  \\\\\n",
        "1 & x^{(n)}_1 & \\dots & x^{(n)}_{m-1} & x^{(n)}_m \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "где $n$ - количество записей данных.\n",
        "\n",
        "> Верхний индекс в нашем представлении означает номер записи в данных.\n",
        "\n",
        "> Помните! Столбец единиц является спецификой для линейной регрессии, поэтому при работе с линейной регрессией и матричном умножении важно не забывать добавлять этот столбец!\n",
        "\n",
        "И обратите внимание, что представление функции предсказания $h_W(X)$ вообще не поменялось! Только результатом функции является уже не скаляр, а вектор $\\hat{y}$:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \n",
        "\\begin{bmatrix}\n",
        "\\hat{y}^{(1)} \\\\\n",
        "\\hat{y}^{(2)} \\\\\n",
        "\\vdots \\\\\n",
        "\\hat{y}^{(n-1)} \\\\\n",
        "\\hat{y}^{(n)} \\\\\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJCUWrjfp9bt"
      },
      "source": [
        "---\n",
        "\n",
        "Для пущего понимания разберем более подробно. Перемножение матриц/векторов делается по правилу \"элемент строки левой матрицы умножаются на элементы столбца правой матрицы, складываются и размещаются в результирующей матрице на строке по индексам строки левой матрицы и столбца правой матрицы\". \n",
        "\n",
        "В общем случае мы берем\n",
        "$$\n",
        "X^{(1)}=\n",
        "\\begin{bmatrix}\n",
        "1 & x^{(1)}_1 & \\dots & x^{(1)}_{m-1} & x^{(1)}_m\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        "\n",
        "и умножаем поэлементно на \n",
        "\n",
        "$$\n",
        "W = \n",
        "\\begin{bmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\vdots \\\\\n",
        "w_{m-1} \\\\\n",
        "w_m\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "то есть\n",
        "\n",
        "$$\n",
        "\\hat{y}^{(1)} = X^{(1)}*W=\n",
        "w_0 + w_1*x^{(1)}_1 + \\dots + w_{m-1}*x^{(1)}_{m-1} + w_m*x^{(1)}_m \n",
        "$$\n",
        "\n",
        "Так мы получаем результат для первой записи вектора предсказаний $\\hat{y}^{(1)}$. При этом, матричное умножение позволяет получить результаты предсказаний для всех данных одной операций (что часто оптимизированно на процессорах и выполняется быстрее, чем проход циклом по каждой записи).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6jTZMRENjHb"
      },
      "source": [
        "# Let`s get our hands dirty! (Начнем практиковаться)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMigy9QaO4t_"
      },
      "source": [
        "Перед тем, как делать какие-либо реализации, необходимо посмотреть, в каком виде представлены наши данные. В данном случае мы имеет матрицу данных `X_data` и вектор истинных значений цены `y_data`. Для самого простого анализа отобразим размерности данных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPA4NAzOP07r",
        "outputId": "44e6420e-c02b-44a1-aa11-7ebc106320f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_data.shape, y_data.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100, 1), (100,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPNGyCMbQI-h"
      },
      "source": [
        "Отлично, мы видим, что данные представлены имеют 100 записей (первая размерность) и всего один признак (вторая размерность). Вектор разметки (истинных значений) обычно представлен одной размерность.\n",
        "\n",
        "После того, как мы знаем размерности, можно отобразить несколько данных из набора:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v93he7zMQQl9",
        "outputId": "cbb82e0e-9575-44a6-cf36-357e3b40ec28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_data[:3]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.02208847],\n",
              "       [1.08233798],\n",
              "       [1.10167651]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJkDM9-hQTVW",
        "outputId": "6aa23ff6-7ee8-47e4-dec6-f4e2bef5bdef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_data[:3]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.24689111, 1.894047  , 1.58552954])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuVwiDBHODtZ"
      },
      "source": [
        "Для начала, нашей задачей является написать реализацию функции предсказания модели. Начнем мы со скалярного представления:\n",
        "\n",
        "> Напомним, $\\hat{y} = w_0 + w*x$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px9ooNzfrEn8"
      },
      "source": [
        "def predict_scalar_one(x, w0, w1):\n",
        "    '''\n",
        "    x - скалярное значение признака\n",
        "    w0 - константное смещение\n",
        "    w1 - вес признака\n",
        "    '''\n",
        "    # TODO - напишите код для вычисления предсказания по одной переменной\n",
        "    y = w0 + w1*x\n",
        "    return y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1Sw7n7pgtZb"
      },
      "source": [
        "# TEST\n",
        "\n",
        "# В таких проверочных блоках будет проверяться корректность\n",
        "#   написания вами кода, так что если в блоке выпадает ошибка - не пугайтесь,\n",
        "#   просто, вероятно, код делает не совсем правильные вещи\n",
        "\n",
        "# assert - ключевое слово Python\n",
        "#   если ему передан True - он просто ничего не делает\n",
        "#   если False - то он выдаст ошибку AssertionError, \n",
        "#                   что означает проверка не пройдена\n",
        "\n",
        "x = 1\n",
        "w0 = 2\n",
        "w1 = 3\n",
        "\n",
        "assert predict_scalar_one(x, w0, w1) == 5"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kycGa5L5OlRf"
      },
      "source": [
        "Теперь воспользуемся данной реализацией, достанем из данных значение признака, предскажем значение на основе случайно выбранных весов и сравним с истинным."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT9-856esTQg",
        "outputId": "33572c24-c368-4c6f-d84a-e0b8863445b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_index = 0\n",
        "\n",
        "x = X_data[data_index, 0]\n",
        "y_true = y_data[data_index]\n",
        "# Зададим для примера следующие веса\n",
        "w0 = 3\n",
        "w1 = 0.2\n",
        "\n",
        "y_pred_sc = predict_scalar_one(x, w0, w1)\n",
        "print(f'y_pred_sc = predict_scalar({x}, {w0}, {w1}) = {y_pred_sc}')\n",
        "print(f'y_true = {y_true}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_pred_sc = predict_scalar(1.0220884684944096, 3, 0.2) = 3.2044176936988817\n",
            "y_true = 1.2468911136328207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrCTGe1nQv_N"
      },
      "source": [
        "Как видно, предсказанное значение далековато от истинного. Теперь попрактикуемся в векторизации для случаев, когда у нас не один признак, а много, то есть $X = \n",
        "\\begin{bmatrix}\n",
        "x_1 & \\dots & x_{m-1} & x_m\n",
        "\\end{bmatrix}$.\n",
        "\n",
        "Напишите функцию предсказания для вектора признаков (одной записи в данных):\n",
        "\n",
        "> Не забудьте, что на выходе мы должны получить скалярное значение, а не массив"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKCEXk14syjr"
      },
      "source": [
        "def predict_one(X, W):\n",
        "    '''\n",
        "    X - вектор признаков [M]\n",
        "    W - вектор весов [M+1]\n",
        "    '''\n",
        "    # TODO - напишите функцию предсказания\n",
        "    mult = 0\n",
        "    for i in range(len(X)):\n",
        "      mult += W[i+1] * X[i]\n",
        "    y = W[0] + mult\n",
        "    return y"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XE4YPn6bH7d"
      },
      "source": [
        "# TEST\n",
        "\n",
        "# Запись в данных имеет, например, три [M = 3] признака\n",
        "X = np.array([1, 2, 0])\n",
        "# Веса модели [M+1]\n",
        "W = np.array([3, 2, 1, 1])\n",
        "y_pred = predict_one(X, W)\n",
        "\n",
        "assert not isinstance(y_pred, (list, tuple, np.ndarray))\n",
        "assert y_pred == 7"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNYWxJGYtuyX",
        "outputId": "37cc0181-3e6b-476c-a733-09477ed20e10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Вектор наших случайно выбранных весов\n",
        "selected_W = np.array([w0, w1])\n",
        "X = X_data[data_index]\n",
        "y_pred_one = predict_one(X, selected_W)\n",
        "print(f'y_pred = predict_one({X}, {selected_W}) = {y_pred_one}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_pred = predict_one([1.02208847], [3.  0.2]) = 3.2044176936988817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiJt6KJguqcn"
      },
      "source": [
        "Соответственно, проверка на равенство должна пройти:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoGz9i_6uTqM"
      },
      "source": [
        "assert y_pred_sc==y_pred_one"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABODZ1lUvzSI"
      },
      "source": [
        "Мы рассмотрели случай, когда у нас всего одна запись в данных. Теперь сыграем серьезно - напишем функцию предсказания для набора данных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0Q4seIJyfew"
      },
      "source": [
        "def predict(X, W):\n",
        "    '''\n",
        "    X - 2D матрица признаков [N, M]\n",
        "    W - вектор весов [M+1]\n",
        "    '''\n",
        "    # TODO - напишите функцию предсказания\n",
        "    # mult_array = 0\n",
        "    # dump = []\n",
        "    # for i in range(len(X[0])):\n",
        "    #   for j in range(len(X[1])):\n",
        "    #     mult_array += W[j+1]*X[i][j]\n",
        "    #   # np.append(y, W[0] + mult_array)\n",
        "    #   dump.append(W[0] + mult_array)\n",
        "    #   mult_array = 0\n",
        "    # y = np.array(dump)\n",
        "    y = W[0] + X @ W[1:]\n",
        "    # y = W[0] + W[1:] @ X\n",
        "    return y"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeVpaC8Lc9Ci"
      },
      "source": [
        "# TEST\n",
        "X = np.array([\n",
        "    [1, 2, 3],\n",
        "    [2, 3, 4],\n",
        "    [3, 4, 5]\n",
        "])\n",
        "W = np.array([3, 2, 1, 2])\n",
        "y_pred = predict(X, W)\n",
        "\n",
        "assert np.all(y_pred == np.array([13, 18, 23]))\n",
        "assert len(y_pred.shape) == 1\n",
        "assert y_pred.shape[0] == 3"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GViMlYVrdSDy"
      },
      "source": [
        "Теперь достанем часть данных и сделаем предсказание в цикле векторной функцией и сравним с нашей матричной реализацией:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "musPsJE-uoPO"
      },
      "source": [
        "data_len = 10\n",
        "\n",
        "X = X_data[10:10+data_len]\n",
        "y_true = y_data[10:10+data_len]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4OPYHNSxvvw",
        "outputId": "78414a2b-4f9b-4685-adb4-e589875fa529",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.29820257],\n",
              "       [1.35397001],\n",
              "       [1.39068846],\n",
              "       [1.43156571],\n",
              "       [1.46347624],\n",
              "       [1.47837698],\n",
              "       [1.48815294],\n",
              "       [1.55797544],\n",
              "       [1.5636969 ],\n",
              "       [1.62397808]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svjrhO2ExvDY"
      },
      "source": [
        "# Для случая нескольких записей при работе без матриц придется проходить\n",
        "#   циклом по каждой записи\n",
        "y_pred_sc = []\n",
        "for x in X:\n",
        "    y = predict_one(x, selected_W)\n",
        "    y_pred_sc.append(y)\n",
        "\n",
        "y_pred_sc = np.array(y_pred_sc)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFxEEB-nwm0d",
        "outputId": "be7f6522-2ebd-4008-e3f3-4e8e8d9dd84d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred_sc"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.25964051, 3.270794  , 3.27813769, 3.28631314, 3.29269525,\n",
              "       3.2956754 , 3.29763059, 3.31159509, 3.31273938, 3.32479562])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcwXLuE6xgDR"
      },
      "source": [
        "y_pred = predict(X, selected_W)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mexKDQ0sTbCM",
        "outputId": "52c64e10-cc66-4ca0-b4ef-4f22efc9e6a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.25964051, 3.270794  , 3.27813769, 3.28631314, 3.29269525,\n",
              "       3.2956754 , 3.29763059, 3.31159509, 3.31273938, 3.32479562])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLp69_bex34b",
        "outputId": "5ce185e6-6678-4952-aeb2-a0a3d31aba0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f'ys_pred == ys_pred_sc -> {np.all(y_pred_sc==y_pred)}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ys_pred == ys_pred_sc -> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANSTohz8ToxP"
      },
      "source": [
        "Отлично! У нас готова функция предсказания модели, которой можно передать набор данных, вектор весов и получить предсказанное значение!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZS_SOMsSadk"
      },
      "source": [
        "# Визуализация предсказаний"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKrLjq8bTyjF"
      },
      "source": [
        "Одним из полезных способов анализа является визуализация предсказаний модели. Для нашего случая это можно сделать без каких-либо предобработок, так как количество признаков небольшое."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3bZbGAnSgNY"
      },
      "source": [
        "def plot_model(X, y_pred, y_true):\n",
        "    plt.scatter(X, y_data, label='Данные')\n",
        "    plt.plot(X, y_pred, 'k--', label='Предсказание модели')\n",
        "    plt.ylabel('$Y$')\n",
        "    plt.xlabel('$X$')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqwN2EbUUx7i",
        "outputId": "5818c5f4-97a1-4685-eac8-2b67475787e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "y_pred = predict(X_data, selected_W)\n",
        "plot_model(X_data, y_pred, y_data)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR5XOYQ8VzVs"
      },
      "source": [
        "На графике видно, что предсказания модели не соотносятся с данными, что говорит о том, что веса подобраны плохо. Это исправит процесс обучения модели!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32YtJrFYyzGd"
      },
      "source": [
        "# Метрики"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDsHXrkoWAVj"
      },
      "source": [
        "Перед тем, как обучать модель, важной составляющей является оценка работы модели не только визуально (что полезно, но чаще всего субъективно), но и по численным характеристикам.\n",
        "\n",
        "> При любых экспериментах рекомендуется отдавать предпочтение сравнению по численным показателям, так как визуально восприятие человека часто бывает искажено."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BLMK2FXWYtr"
      },
      "source": [
        "В данной практике начнем знакомтсво с двумя показателями: Корень среднеквадратичного отклонения (Root Mean Squared Error) и Среднее абсолютное отклонение (Mean Absolute Error). Обе характеристики имеют диапазон значений [0; $\\infty$) и оцениваются по принципу \"меньше значение метрики - лучше модель работает\".\n",
        "\n",
        "Квадратичное отклонение возводит значения ошибок в квадрат, поэтому сильнее реагирует на большие ошибки, так что его лучше применять в случае, когда большие ошибки нежелательны. Малые отклонения, возведенные в квадрат, наоборот становятся еще меньше. \n",
        "\n",
        "Абсолютные отклонения в свою очередь не меняют величину отклонений и представляют буквально среднее значение ошибок."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXhBh_QY8rZB"
      },
      "source": [
        "\n",
        "$$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (h_W(x^{(i)})-y^{(i)})^2 } $$\n",
        "\n",
        "$$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |h_W(x^{(i)})-y^{(i)}| $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h-EkSlxY9kD"
      },
      "source": [
        "Пора перейти к реализации, давайте напишем функции для оценки предсказаний:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-ALO8fIx7ql"
      },
      "source": [
        "def rmse_score(y_true, y_pred):\n",
        "    '''\n",
        "    y_true - вектор истинных значений\n",
        "    y_pred - вектор предсказанных значений\n",
        "    '''\n",
        "    # TODO - напишите функцию вычисления RMSE\n",
        "    my_sum = 0\n",
        "    for i in range(len(y_pred)):\n",
        "      my_sum += (y_pred[i] - y_true[i])**2\n",
        "    rmse_value = (my_sum/len(y_pred))**0.5\n",
        "    return rmse_value\n",
        "\n",
        "def mae_score(y_true, y_pred):\n",
        "    '''\n",
        "    y_true - вектор истинных значений\n",
        "    y_pred - вектор предсказанных значений\n",
        "    '''\n",
        "    # TODO - напишите функцию вычисления MAE\n",
        "    my_sum = 0\n",
        "    for i in range(len(y_pred)):\n",
        "        my_sum += abs(y_pred[i] - y_true[i])\n",
        "    # print(np.sum(my_sum))\n",
        "    mae_value = my_sum / len(y_pred)\n",
        "    return mae_value"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "780ycb7ghlSF",
        "outputId": "1dda22a0-11cb-4a51-90bd-a969ef1ab00b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TEST\n",
        "y_true = np.array([0.5, 0.1, -0.4, 1.2])\n",
        "y_pred = np.array([0.2, 0.0, -0.1, 3.1])\n",
        "\n",
        "rmse_value = rmse_score(y_true, y_pred)\n",
        "mae_value = mae_score(y_true, y_pred)\n",
        "print(rmse_value, mae_value)\n",
        "\n",
        "assert np.isclose(rmse_value, 0.974679)\n",
        "assert np.isclose(mae_value, 0.65)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9746794344808964 0.65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOTm_-V1ZRv7"
      },
      "source": [
        "Для проверки оценим работу модели на всем наборе данных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZjtcY5DzJdo",
        "outputId": "1d4de457-148e-4fed-823e-d0f3239580ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Проверим ошибку на всем наборе данных\n",
        "y_true = y_data\n",
        "X = X_data\n",
        "y_pred = predict(X, selected_W)\n",
        "\n",
        "rmse_value = rmse_score(y_true, y_pred)\n",
        "mae_value = mae_score(y_true, y_pred)\n",
        "\n",
        "rmse_value, mae_value"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8822495430466522, 0.7395733955141037)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBWB7Oym7TvO"
      },
      "source": [
        "> Существует также представление метрики $MSE$, которая имеет отношение с $RMSE$:\n",
        "$$\n",
        "RMSE = \\sqrt{MSE}\n",
        "$$\n",
        "По сути чистое $MSE$ не соотносится по порядкам значений с $MAE$, поэтому в качестве метрики редко используется.\n",
        "\n",
        "Осталось запомнить эти значения, чтобы затем сравнить с метриками на обученной модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5vKlErN0hEM"
      },
      "source": [
        "# Функция потерь"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2So48ezJZ44R"
      },
      "source": [
        "Процесс обучения модели заключается в том, что мы каким-то образом меняем параметры модели (в случае линейной регрессии - веса признаков $w_i$ и смещение $w_0$) так, чтобы в результате обучения наша модель предсказывала данные как можно точнее. Для реализации такого процесса применяют методы оптимизации.\n",
        "\n",
        "> Напомним, задача оптимизации - поиск экстремума (оптимума) по определенному критерию (функции потерь).\n",
        "\n",
        "Мы помним, что при разных параметрах модели предсказания то лучше предсказывает, то хуже. То есть, функция потерь $J$ должна отражать, насколько хорошо модель предсказывает. Ну и так как мы меняем веса модели - это значит, что оптимизируем мы в пространстве параметров $W$. Значит обозначение функции потерь в нашем случае будет $J(W)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv4pAK_maqOC"
      },
      "source": [
        "Для обучения модели линейной регрессии применяется следующая функция потерь, которую стараются минимизировать:\n",
        "\n",
        "$$J(W) = \\frac{1}{2*n} \\|h_W(X)-y\\|^2_2 $$\n",
        "\n",
        "или\n",
        "\n",
        "$$J(W) = \\frac{1}{2*n} \\sum_{i=1}^{n} (h_W(x^{(i)})-y^{(i)})^2 $$\n",
        "\n",
        "где $y$ - значение зависимой переменной в данных, $\\|.\\|_2$ - $L2$ norm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td_k_bRaajHG"
      },
      "source": [
        "Данная функция потерь очень похожа на метрику $MSE$ за одним отличием - в знаменателе имеется число 2, что приводит к виду $J(W) = MSE/2$.\n",
        "\n",
        "Главные свойства, которые должна иметь функция потерь:\n",
        "- Отражать зависимость поведения модели: \"меньше - лучше\";\n",
        "- Быть дифференцируемой.\n",
        "\n",
        "Так как мы помним, что $MSE$ так и отражает принцип \"меньше значение метрики - лучше модель работает\", то доказывать дифференцируемость функции в данным момент не будем. Данная функция полностью удовлетворяет требованиям и можно заняться ее реализацией.\n",
        "\n",
        "Но зачем же тут двойка? На самом деле, она упростит нам жизнь и сократится с другой двойкой при выводе формулы производной. Давайте проверим, не испортит ли она картину здесь:\n",
        "\n",
        "Допустим, что мы имеет две модели - $M_1$ и $M_2$. Назначение функции потерь (как и метрики $MSE$) - оценить, насколько хорошо/плохо работает модель относительно другой модели. Положим, что сейчас показатели $MSE$ следующие:\n",
        "- $MSE_{M_1} = 10$;\n",
        "- $MSE_{M_2} = 6$;\n",
        "\n",
        "Мы видим, что $M_2$ имеет меньшее значение, а значит работает лучше, чем $M_1$. А теперь, что будет, если мы оба значения поделим на 2? Посмотрим:\n",
        "- $MSE_{M_1} = 5$;\n",
        "- $MSE_{M_2} = 3$;\n",
        "\n",
        "Фактически, изменились оба значения, но результат оценки все тот же - $M_2$ работает лучше, чем $M_1$! Это приводит нас к выводу, что метрика $MSE$ (как и остальные - $MAE$, $RMSE$) не искажается (не меняет результатов оценки) при масштабировании коэффициентом (в нашем случае $\\frac{1}{2}$)!\n",
        "\n",
        "Давайте теперь напишем реализации наших функций потерь!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pulArCQsehPK"
      },
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "    '''\n",
        "    y_true - вектор истинных значений\n",
        "    y_pred - вектор предсказанных значений\n",
        "    '''\n",
        "    # TODO - напишите реализацию функции потерь\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu2FF0cSik6N"
      },
      "source": [
        "# TEST\n",
        "y_true = np.array([0.5, 0.1, -0.4, 1.2])\n",
        "y_pred = np.array([0.2, 0.0, -0.1, 3.1])\n",
        "\n",
        "rmse_value = rmse_score(y_true, y_pred)\n",
        "loss_value = loss_function(y_true, y_pred)\n",
        "\n",
        "assert np.isclose(loss_value, rmse_value**2/2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26lgVRiSzWzL"
      },
      "source": [
        "Соответственно, проверим наши данные: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6kvzMNGb2jQ"
      },
      "source": [
        "# Вычислим показатель loss на наших данных\n",
        "y_true = y_data\n",
        "X = X_data\n",
        "y_pred = predict(X, selected_W)\n",
        "\n",
        "loss_value = loss_function(y_true, y_pred)\n",
        "loss_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuYm0gAylH4X"
      },
      "source": [
        "Для понимания отобразим пространство весов (так как всего два параметра - мы можем это сделать!) и визуализируем поверхность функции потерь. Также отобразим точками положение в пространстве, которое соответсвует весам, по которым данные были построены (веса истинно зависимости данных), а также веса, которые были заданы случайно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdDSkLKe1EHq"
      },
      "source": [
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import axes3d\n",
        "\n",
        "side_sz = 300\n",
        "\n",
        "w0_vals = np.linspace(0, 3.5, side_sz)\n",
        "w1_vals = np.linspace(0, 1, side_sz)\n",
        "\n",
        "X = X_data\n",
        "y_true = y_data\n",
        "\n",
        "Z_MAX = 0.6\n",
        "\n",
        "losses = np.ndarray((side_sz, side_sz))\n",
        "for j in range(len(w1_vals)):\n",
        "    w1 = w1_vals[j]\n",
        "    for i in range(len(w0_vals)):\n",
        "        w0 = w0_vals[i]\n",
        "        \n",
        "        render_W = np.array([w0, w1])\n",
        "        y_pred = predict(X, render_W)\n",
        "\n",
        "        loss = loss_function(y_true, y_pred)\n",
        "        loss = np.clip(loss, 0, Z_MAX)\n",
        "        losses[j, i] = loss\n",
        "\n",
        "y_pred = predict(X, np.array(real_W))\n",
        "real_point_loss = loss_function(y_true, y_pred)\n",
        "\n",
        "y_pred = predict(X, selected_W)\n",
        "current_point_loss = loss_function(y_true, y_pred)\n",
        "\n",
        "fig = plt.figure(figsize=[15,15])\n",
        "views = [\n",
        "    # Subplot code, (around X, around Z)\n",
        "    (221, (60, -60)),\n",
        "    (222, (0, -5)),\n",
        "    (223, (0, -90)),\n",
        "]\n",
        "\n",
        "ww0, ww1 = np.meshgrid(w0_vals, w1_vals)\n",
        "\n",
        "for view in views:\n",
        "    ax = fig.add_subplot(view[0], projection='3d')\n",
        "    ax.plot_wireframe(\n",
        "        ww0, \n",
        "        ww1, \n",
        "        losses, \n",
        "        color='lightblue', \n",
        "        rstride=8, \n",
        "        cstride=8, \n",
        "        label='Loss',\n",
        "        cmap=cm.coolwarm,\n",
        "    )\n",
        "\n",
        "    ax.scatter(\n",
        "        xs = [real_W[0]],\n",
        "        ys = [real_W[1]],\n",
        "        zs = [real_point_loss],\n",
        "        color='r',\n",
        "        label='Best $W$',\n",
        "        s=100\n",
        "    )\n",
        "\n",
        "    ax.scatter(\n",
        "        xs = [selected_W[0]],\n",
        "        ys = [selected_W[1]],\n",
        "        zs = [current_point_loss],\n",
        "        color='g',\n",
        "        label='Current $W$',\n",
        "        s=100\n",
        "    )\n",
        "\n",
        "    ax.view_init(*view[1])\n",
        "    \n",
        "    font = {\n",
        "        'color': 'black',\n",
        "        'weight': 'bold',\n",
        "        'size': 24,\n",
        "    }\n",
        "    ax.set_xlabel('$w_0$', fontdict=font)\n",
        "    ax.set_ylabel('$w_1$', fontdict=font)\n",
        "    ax.set_zlabel('J($W$)', fontdict=font)\n",
        "    \n",
        "    # ax.tick_params(axis='x', colors='black')\n",
        "    # ax.tick_params(axis='y', colors='black')\n",
        "    # ax.tick_params(axis='z', colors='black')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg6VXx8DvTGJ"
      },
      "source": [
        "График представляет синюю плоскость, которая является плоскостью функции потерь. То есть, если мы будем двигаться по осям и менять веса, то будем попадать на разные точки на этой плоскости. У каждого места на этой плоскости своя высота - значение функции потерь, а нам нужно спуститься в самую низкую точку!\n",
        "\n",
        "> В визуализации специально ограничено максимальное значение функции потерь (из-за этого такие плоские области сверху) для более наглядной демонстрации минимума.\n",
        "\n",
        "Как видно на графиках (представление с трех сторон для понимания), нынешние веса располагаются в синей точке, а красная точка - точка наилучших весов. Можно заметить, что точка наилучших весов является минимумом данной поверхности.\n",
        "\n",
        "Таким образом, задача обучения - перейти из синей точки в красную точку.\n",
        "\n",
        "Давайте теперь для упрощения глянем на двумерный график, если 3D был сложен для понимания:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffHJ8bD32zZY"
      },
      "source": [
        "w1 = np.linspace(0, 1, 100)\n",
        "w0 = real_W[0]\n",
        "\n",
        "X = X_data\n",
        "y_true = y_data\n",
        "\n",
        "losses = []\n",
        "\n",
        "for w1_val in w1:\n",
        "    render_W = np.array([w0, w1_val])\n",
        "    y_pred = predict(X, render_W)\n",
        "\n",
        "    loss = loss_function(y_true, y_pred)\n",
        "    # loss = np.clip(loss, 0, Z_MAX)\n",
        "    losses.append(loss)\n",
        "\n",
        "losses = np.array(losses)\n",
        "\n",
        "plt.plot(w1, losses)\n",
        "\n",
        "# Render best point\n",
        "y_pred = predict(X, np.array([w0, real_W[1]]))\n",
        "best_loss = loss_function(y_true, y_pred)\n",
        "plt.scatter([real_W[1]], [best_loss], color='r', s=100, label='Best $w_1$')\n",
        "\n",
        "# Render selected point\n",
        "y_pred = predict(X, np.array([w0, selected_W[1]]))\n",
        "selected_loss = loss_function(y_true, y_pred)\n",
        "plt.scatter([selected_W[1]], [selected_loss], color='g', s=100, label='Best $w_1$')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel('$w_1$')\n",
        "plt.ylabel('$J(W)$')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPxv25xt5NWE"
      },
      "source": [
        "Здесь мы представили простой случай, когда мы меняем только вес признака, а константтное смещение не меняется, но и здесь видно, что задачей обучения будет спуск из зеленой точки (нынешний вес) до красной точки (лучший вес - минимум функции потерь)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_YYgTh_0okm"
      },
      "source": [
        "# Градиентный спуск"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cho2qULHVbXI"
      },
      "source": [
        "Обучение моделей может быть сделано с использованием различных алгоритмов оптимизации, но одним из наиболее простых и в то же время эффективных показал себя алгоритм **градиентного спуска**. Основная идея алгоритма заключена в определении градиента (вектора производных).\n",
        "\n",
        "Для примера возьмем функцию $y=0.5x^2$ и точку $x=1$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWW5EAn3VbXJ"
      },
      "source": [
        "x = np.linspace(-3, 3, 100)\n",
        "y = 0.5*x**2\n",
        "\n",
        "x_grad = np.linspace(0, 2, 100)\n",
        "y_grad = x_grad-0.5\n",
        "\n",
        "plt.plot(x, y, 'b')\n",
        "plt.plot(x_grad, y_grad, 'g--')\n",
        "plt.scatter([1], [0.5], s=100)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQNDX9a9xafQ"
      },
      "source": [
        "На графике синей линией обозначен график функции, а зеленым пунктиром - касательная в точке. Как помним, касательная - это как раз график производной функции.\n",
        "\n",
        "По сути производная функции в точке показывает, **как** изменится значение функции при изменении аргумента, по которому берется производная, от этой точки. Производная в точке $x=1$ есть $\\frac{\\partial{(0.5x^2)}}{\\partial{x}} = 1x = 1$. То есть, судя по производной (прямой линии), увеличим аргумент $x$ на единицу ($x=2$) и значение функции должно измениться до $y=1.5$. Судя по функции, изменение происходит по-другому ($x=2, y=2$). По такому примеру видно, что имея значение производной в точке, можно оценить, **в какую сторону** изменится значение функции при изменении аргумента.\n",
        "\n",
        "Попробуем подойти с другой стороны: имеем описание функции $y=0.5x^2$ и точку, в которой мы располагаемся $x=1$. Попробуем найти ответ на вопрос, как нужно изменить значение $x$, чтобы **уменьшить значение функции** $y=f(x)$. Попробуем проверить два простых варианта:\n",
        "- к значению $x$ прибавим значение производной в точке;\n",
        "- от значения $x$ отнимем значение производной в точке.\n",
        "\n",
        "Первый вариант $x \\leftarrow x + \\frac{\\partial}{\\partial x} f(x)$ нам дает следующий расчет, производная в точке равна $\\frac{\\partial}{\\partial x}f(x) = 1$, прибавление значения дает нам $x=2$, при котором $f(x)=2$. То есть, прибавление производной дало нам увеличение значения функции.\n",
        "\n",
        "Во втором варианте $x \\leftarrow x - \\frac{\\partial}{\\partial x} f(x)$ мы имеет новое значение аргумента $x=0$, при котором $f(x) = 0$. То есть, вычитание значения производной позволило уменьшить значение функции.\n",
        "\n",
        "> Можете проверить аналогично при $x=-2$, прибавление производной ведет к увеличению функции (движемся влево), а вычитание ведет к уменьшению.\n",
        "\n",
        "Таким образом, мы выяснили, что производная в точке показывает направление изменения функции при перемещении из этой точки, поэтому мы этим пользуемся для градиентных методов:\n",
        "- градиентный спуск (gradient descent) - хотим двигаться к минимуму функции (минимизация), поэтому для уменьшения значения функции от аргумента отнимаем значение производной;\n",
        "- градиентный подъем (gradient ascent) - хотим двигаться к максимуму функции (максимизация), поэтому для увеличения значения функции к аргументу прибавляем значение производной."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF0jmtQHVbXL"
      },
      "source": [
        "Введем небольшую поправку, возьмем функцию $y=5x^2$, производная равна $\\frac{\\partial y}{\\partial x} = 10x$. Если мы находимся в точке $x=1, y=5$ и вычтем производную из аргумента, то получим $x=-9$, что привело к увеличению функции $y=405$. Странно, правило не работает?\n",
        "\n",
        "> При этом можете убедиться, что прибавление производной также ведет к увеличению значения функции.\n",
        "\n",
        "Да, в этом случае мы имеем слишком **большое значение градиента** и это приводит к эффекту **расходящегося градиента**. Это такой эффект, когда мы пытаемся двигаться к уменьшению функции, а из-за большого значения градиента наше перемещения наоборот увеличивает значение функции.\n",
        "\n",
        "Для этого в правила градиентного спуска/подъема вводится специальный коэффициент $\\alpha$ и правило для спуска выглядит так:\n",
        "$$\n",
        "x \\leftarrow x - \\alpha \\frac{\\partial}{\\partial x} f(x)\n",
        "$$\n",
        "\n",
        "Этот коэффициент позволяет уменьшить значени градиента, чтобы он не так сильно влиял на перемещение. Так для последнего случая, имея $\\alpha=0.1$, вычитание даст переход из точки $x=1, y=5$ при $\\alpha \\frac{\\partial y}{\\partial x} = 1$ в точку $x=0, y=0$. Отлично, теперь правило снова работает!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Adz6CjqVbXM"
      },
      "source": [
        "Отлично! Понимая, как нужно менять значение аргумента функции, чтобы эту самую функцию минимизировать, мы просто делаем это последовательно шаг за шагом, чтобы так и добраться до минимума. Это очень *жадное* предположение (с точки зрения алгоритмов), но зато алгоритм очень простой и требует лишь нахождения производной функции.\n",
        "\n",
        "Теперь обратимся к нашей задаче. У нас есть:\n",
        "- функция потерь $J(W)$, которая оценивает, как работает наша модель;\n",
        "- параметры модели $W$, которые мы меняем, чтобы сделать работу модели лучше/хуже.\n",
        "\n",
        "Таким образом, используя принцип градиентного спуска для минимизации функции потерь, мы меняем аргумент в соответствии с правилом градиентного спуска, вычитая значение производной умноженной на $\\alpha$ (который в обучении называется **коэффициентом обучения**):\n",
        "$$\n",
        "W \\leftarrow W - \\alpha \\frac{\\partial}{\\partial W} J(W)\n",
        "$$\n",
        "\n",
        "Но мы помним, что $W$ - это не скаляр, а вектор, так что правило обновления для каждого веса будет выглядеть так:\n",
        "$$\n",
        "w_j \\leftarrow w_j - \\alpha \\frac{\\partial}{\\partial w_j} J(W)\n",
        "$$\n",
        "\n",
        "где $w_j$ - $j-й$ вес.\n",
        "\n",
        "По данной формуле видно, что мы просто выводим формулу производной функции, а потом вычисляем значение функции потерь при нынешних весах, обновляем веса и так шаг за шагом. Перейдем к выводу производной!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XbMxrZbgqBO"
      },
      "source": [
        "Еще раз вспомним, как выглядит функция потерь и функция предсказания для линейной регрессии для одной переменной:\n",
        "\n",
        "$$\n",
        "J(W) = \\frac{1}{2*n} \\sum_{i=1}^{n} (h_W(x^{(i)})-y^{(i)})^2 \\\\\n",
        "h_W(X) = XW = \\sum_{i=1}^{n} x^{(i)}*w_1+w_0\n",
        "$$ \n",
        "\n",
        "Для дальнейшего вывода введем упрощающее обозначение:\n",
        "$$\n",
        "E(x^{(i)}) = h_W(x^{(i)})-y^{(i)}\n",
        "$$\n",
        "\n",
        "Значит производная функции потерь будет выглядеть следующим образом:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_j} J(W) = \n",
        "\\frac{\\partial}{\\partial w_j}  \\frac{1}{2*n} \\sum_{i=1}^{n} E(x^{(i)})^2 =\n",
        "\\frac{1}{2*n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial w_j} E(x^{(i)})^2\n",
        "$$\n",
        "\n",
        "Данный вывод сделали с учетом правила работы:\n",
        "- С производными от умножения на скаляры $\\frac{\\partial}{\\partial x}kx = k\\frac{\\partial}{\\partial x}x$;\n",
        "- Производными сумм $\\frac{\\partial}{\\partial x}(f(x)+g(x)) = \\frac{\\partial}{\\partial x}f(x) + \\frac{\\partial}{\\partial x}g(x)$.\n",
        "\n",
        "Далее требуется найти производную квадрата разницы между предсказанным и истинным значениями:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_j} E(x^{(i)})^2 = \\\\\n",
        "2*E(x^{(i)})\\frac{\\partial}{\\partial w_j} E(x^{(i)})\n",
        "$$\n",
        "\n",
        "Для данного вывода мы воспользовались:\n",
        "- Цепным правилом $\\frac{\\partial}{\\partial x}f(g(x)) = \\frac{\\partial}{\\partial g(x)}f(g(x)) * \\frac{\\partial}{\\partial x}g(x)$.\n",
        "- Правилом степеней $\\frac{\\partial}{\\partial x}x^n = n*x^{n-1}\\frac{\\partial}{\\partial x}x$;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoKkNgYpixId"
      },
      "source": [
        "Промежуточный вид с учетом $E(x^{(i)}) = h_W(x^{(i)})-y^{(i)}$ (разворачиваем обратно и под производной разворачиваем $h_W(x^{(i)})$ к виду линейной регрессии):\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_j} J(W) = \n",
        "\\frac{1}{2*n} \\sum_{i=1}^{n} 2*(h_W(x^{(i)})-y^{(i)})\\frac{\\partial}{\\partial w_j}((x^{(i)}*w_1+w_0)-y^{(i)})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPNB0qPHij4j"
      },
      "source": [
        "\n",
        "Далее находим производные оставшихся частей в зависимости от веса с учетом вида функции предсказания:\n",
        "> Обратите внимание, уже конкретные $w_1$ и $w_0$ вместо $w_j$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_0}((w_0 + w_1*x^{(i)})-y^{(i)}) = 1\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_1}((w_0 + w_1*x^{(i)})-y^{(i)}) = x^{(i)}\n",
        "$$\n",
        "\n",
        "Соответвенно, можно представить вид производных по каждому весу: \n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_0} J(W) = \n",
        "\\frac{1}{n} \\sum_{i=1}^{n} (h_W(x^{(i)})-y^{(i)}) \\\\\n",
        "\\frac{\\partial}{\\partial w_1} J(W) = \n",
        "\\frac{1}{n} \\sum_{i=1}^{n} (h_W(x^{(i)})-y^{(i)})*x^{(i)}\n",
        "$$\n",
        "\n",
        "Можно также это выразить, как матричное выражение (заменив знак суммы на умножение матриц):\n",
        "\n",
        "> Помним, что $X$ - матрица $(n, 1)$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_0} J(W) = \\frac{1}{n} 1^T*(h_W(X)-y) \\\\\n",
        "\\frac{\\partial}{\\partial w_1} J(W) = \\frac{1}{n} X^T*(h_W(X)-y)\n",
        "$$\n",
        "\n",
        "> Здесь 1 - вектор-столбец, состоящий из единиц длиной $n$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFohIdUl01WA"
      },
      "source": [
        "Ура, мы получили математический вид производных, которые позволят нам реализовать алгоритм градиентного спуска для обучения нашей модели!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmUId9kFjBuJ"
      },
      "source": [
        "На всякий случай, более простая визуализации алгоритма, только уже с одним параметром:\n",
        "\n",
        "> В линейной регрессии с одним признаком - два параметра для оптимизации: $w_1$ и $w_0$, здесь в примере просто описание градиентного спуска с единственным параметром"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLvDEb1Zx_Cq"
      },
      "source": [
        "![as](https://miro.medium.com/max/1005/1*f0CuPDSWFUr9XGESWQ4JUA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B8p0JBBjJEr"
      },
      "source": [
        "Суть алгоритма заключается в том, что шарик, находясь в определенном положении (вектор весов $W$ - так как двигаемся мы в пространстве весов), может определить не только, насколько он высоко находится (функция потерь $J(W)$), но и понять, куда ему надо двигаться, чтобы эту высоту уменьшить ($\\frac{\\partial}{\\partial w_j} J(W)$ - градиент, направление движения). Также коэффициент обучения $\\alpha$ говорит о том, насколько большой шаг мы сделаем, огромный и быстрый или маленький и медленный. Цель алгоритма, спустить шарик так низко, насколько это возможно (найти оптимальное значение весов $W$, при котором $J(W)$ будет минимальна)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-yMYHM2kEhI"
      },
      "source": [
        "Для практики реализуем функции вычисления производных для каждого веса по отдельности:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_UrkTwBzQEI"
      },
      "source": [
        "def loss_function_deriv_w0(X, W, y_true):\n",
        "    # TODO - код расчета производной для смещения\n",
        "    return loss_deriv_w0\n",
        "\n",
        "def loss_function_deriv_w1(X, W, y_true):\n",
        "    # TODO - код расчета производной для веса признака 1\n",
        "    return loss_deriv_w1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhy-pl-UkBdz"
      },
      "source": [
        "# TEST\n",
        "X = np.array([\n",
        "    [1],\n",
        "    [4],\n",
        "])\n",
        "W = np.array([1, 2])\n",
        "y_true = np.array([1, 3])\n",
        "\n",
        "dJ_w0 = loss_function_deriv_w0(X, W, y_true)\n",
        "dJ_w1 = loss_function_deriv_w1(X, W, y_true)\n",
        "\n",
        "assert dJ_w1 == 13\n",
        "assert dJ_w0 == 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og_pTL16kK-x"
      },
      "source": [
        "# Посмотрим на данных\n",
        "y_true = y_data\n",
        "X = X_data\n",
        "\n",
        "dJ_w0 = loss_function_deriv_w0(X, selected_W, y_true)\n",
        "dJ_w1 = loss_function_deriv_w1(X, selected_W, y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTJc1f3ie1ip"
      },
      "source": [
        "dJ_w1, dJ_w0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_mRXbKikMiH"
      },
      "source": [
        "После чего, выполним алгоритм поиска на нескольких итерациях, попутно собирая значения функции потерь, чтобы отобразить:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJT6BRyxdMvN"
      },
      "source": [
        "# Alpha - learning rate (lr)\n",
        "lr = 0.01\n",
        "n_iterations = 1000\n",
        "# Создаем вектор весов с размерностью количества признаков + 1\n",
        "new_W = np.zeros(X.shape[1]+1)\n",
        "loss_history = []\n",
        "\n",
        "for i_iter in range(n_iterations):\n",
        "    dJ_w0 = loss_function_deriv_w0(X, new_W, y_true)\n",
        "    dJ_w1 = loss_function_deriv_w1(X, new_W, y_true)\n",
        "    # TODO - код обновления весов\n",
        "\n",
        "    y_pred = predict(X, new_W)\n",
        "    loss = loss_function(y_true, y_pred)\n",
        "    loss_history.append(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_rnzYz1lVdw"
      },
      "source": [
        "# TEST\n",
        "assert np.all(np.isclose(new_W, np.array([0.76555, 0.77251])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMDUQLyb6Vo1"
      },
      "source": [
        "Теперь напишем функцию отображения истории обучения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UvHF4SYgJJk"
      },
      "source": [
        "def show_loss(loss_history):\n",
        "    plt.plot(loss_history)\n",
        "    plt.grid()\n",
        "    plt.title('Loss history')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('$J(X)$')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d7Vq6j36iVQ"
      },
      "source": [
        "show_loss(loss_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBBhDjuYd9Bm"
      },
      "source": [
        "new_W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zMf83lj6xvf"
      },
      "source": [
        "Новые веса уже ближе располагаются к тем весам, по которым построены данные - отлично! Значит алгоритм работает! Осталось отобразить предсказания модели и проверить метрики, чтобы окончательно убедиться:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_tzzHM-gogb"
      },
      "source": [
        "y_pred = predict(X, new_W)\n",
        "plot_model(X_data, y_pred, y_data)\n",
        "\n",
        "rmse_value = rmse_score(y_true, y_pred)\n",
        "rmse_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j06rXs4h7cbd"
      },
      "source": [
        "Метрика $RMSE$ стала меньше, не так ли? Это отличный показатель того, что обученная модель работает лучше, чем с какими-то случайными весами!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S1tLQbu6nUq"
      },
      "source": [
        "## Больше визуализации результатов!\n",
        "\n",
        "Для расширения багажа инструментов анализа познакомимся с еще двумя способами оценки результатов предсказания!\n",
        "\n",
        "Первым способом является отображение распределения отклонений:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBZXTRDX627y"
      },
      "source": [
        "y_residuals = y_true-y_pred\n",
        "\n",
        "sns.distplot(y_residuals, bins=15)\n",
        "plt.yticks([])\n",
        "plt.grid(True)\n",
        "plt.xlabel('Отклонения')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK-2rQ5X7NYw"
      },
      "source": [
        "Мы видим, что распределение является похожим на нормальное и среднее близко к нулю - это важное понимание того, что модель работает правильно! Для сравнения проверим картину со случайными весами:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSqDr-bq7qtn"
      },
      "source": [
        "y_residuals = y_true-predict(X, selected_W)\n",
        "\n",
        "sns.distplot(y_residuals, bins=15)\n",
        "plt.yticks([])\n",
        "plt.grid(True)\n",
        "plt.xlabel('Отклонения')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DabGEWS_70LH"
      },
      "source": [
        "В данном случае характер распределения данных все еще похож на нормальный (из-за того, что данных всего 100 записей), но среднее уже смещено относительно нуля. Это объясняет точечный график отклонений: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfjAsw7T8NzW"
      },
      "source": [
        "_, ax = plt.subplots(1, 2, sharey=True, figsize=[20, 10])\n",
        "\n",
        "y_pred = predict(X, new_W)\n",
        "ax[0].scatter(X, y_true-y_pred)\n",
        "ax[0].grid(True)\n",
        "ax[0].set_title('Trained weights')\n",
        "ax[0].set_xlabel('$X$')\n",
        "ax[0].set_ylabel('Отклонения')\n",
        "\n",
        "y_pred = predict(X, selected_W)\n",
        "ax[1].scatter(X, y_true-y_pred)\n",
        "ax[1].grid(True)\n",
        "ax[1].set_title('Random weights')\n",
        "ax[1].set_xlabel('$X$')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc-jpC_H8NH6"
      },
      "source": [
        "На левом графике обученная модель и отклонения предсказания колеблятся около нуля - это и проверяет картина распределения. В случае плохой модели (график справа) отклонения не сконцентрированы около нуля - модель плохо повторяет зависимость в данных.\n",
        "\n",
        "Второй способ визуализации также является очень интуитивным, это график предсказанных значений от истинных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9G2XcPM9aq2"
      },
      "source": [
        "_, ax = plt.subplots(1, 2, sharey=True, figsize=[20, 10])\n",
        "\n",
        "y_diag = np.sort(y_true)\n",
        "\n",
        "y_pred = predict(X, new_W)\n",
        "ax[0].plot(y_diag, y_diag)\n",
        "ax[0].scatter(y_pred, y_true)\n",
        "ax[0].grid(True)\n",
        "ax[0].set_title('Trained weights')\n",
        "ax[0].set_ylabel('Истинные значения')\n",
        "ax[0].set_xlabel('Предсказания')\n",
        "\n",
        "y_pred = predict(X, selected_W)\n",
        "ax[1].plot(y_diag, y_diag)\n",
        "ax[1].scatter(y_pred, y_true)\n",
        "ax[1].grid(True)\n",
        "ax[1].set_title('Random weights')\n",
        "ax[1].set_xlabel('Предсказания')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZV4PfMZ98eH"
      },
      "source": [
        "Такой график показывает, как распределены предсказания относительно истинных значений. Можно заметить, что при хорошей работе модели (слева) точки повторяют диагональную прямую, а при плохой (справа) - имеют другое распределение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlFGfADEktQr"
      },
      "source": [
        "# Векторный градиентный спуск"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-dm4de-7vrM"
      },
      "source": [
        "Реализованный градиентный спуск работает и этим нельзя не гордиться! Остается вопрос в том, что сейчас реализация ограничена моделью линейной регрессии с одним признаком. Пора бы перейти к общему случаю для любого количества признаков!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrUcXjDfkwyZ"
      },
      "source": [
        "Матричный вид правила обновления выглядит несложно, нужно просто умножить вектор частных производных (градиент) на $\\alpha$ и отнять от старых весов, чтобы получить новые:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "w_0 \\\\\n",
        "w_1\n",
        "\\end{bmatrix}\n",
        "\\leftarrow \n",
        "\\begin{bmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\end{bmatrix}\n",
        "-\n",
        "\\alpha \n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial}{\\partial w_0} J(W) \\\\\n",
        "\\frac{\\partial}{\\partial w_1} J(W) \\\\\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xBMmm9mmeiL"
      },
      "source": [
        "Вспомним, как выглядят правила обновления весов по отдельности, но в матричной форме для случая линейной регрессии одной переменной:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_0} J(W) = \\frac{1}{n} 1^T*(h_W(X)-y) \\\\\n",
        "\\frac{\\partial}{\\partial w_1} J(W) = \\frac{1}{n} X^T*(h_W(X)-y)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Guw7djTn-W5q"
      },
      "source": [
        "Выражение $h_W(X)-y$ в результате дает вектор-столбец, который затем в случае $w_1$ мы поэлементно умножаем на вектор $X$ (матрицу, но у нас же $(n, 1)$). По факту это умножение - перемножение отклонения предсказанного значения для $i$-й записи в данных на $i$-e значение нашего единственного признака. Для случая многих признаков мы бы имели форму:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_j} J(W) = \\frac{1}{n} X^T_j(h_W(X)-y)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHX1Puf8_D9g"
      },
      "source": [
        "То есть, в зависимости от того, по какому весу берется производная, ту колонку (признак) в данных мы и умножаем на вектор отклонений.\n",
        "\n",
        "Теперь вспоминаем, что для $w_0$ нам нужно умножить последнюю колонку матрицы $X$, для $w_1$ - предпоследнюю, и т.д. Значит, чтобы сделать умножение колонок матрицы $X$ на вектор-колонку $h_W(X)-y$ мы можем просто транспонировать матрицу $X$, чтобы умножение делалось не колонка на колонку, а строка на колонку.\n",
        "\n",
        "> В матрице $X^T$ следующая разметка, строки - признаки, колонки - записи.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrS_p5gTo4j_"
      },
      "source": [
        "Таким образом мы получаем для нашего случая:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial}{\\partial w_0} J(W) \\\\\n",
        "\\frac{\\partial}{\\partial w_1} J(W) \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\frac{1}{n} X^T(h_W(X)-y)\n",
        "$$\n",
        "\n",
        "И для общего случая:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial}{\\partial w_0} J(W) \\\\\n",
        "\\frac{\\partial}{\\partial w_1} J(W) \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial}{\\partial w_{m-1}} J(W) \\\\\n",
        "\\frac{\\partial}{\\partial w_m} J(W) \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\frac{1}{n} X^T(h_W(X)-y)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olKAYMT7DMnQ"
      },
      "source": [
        "Самое время провести реализацию выведенных законов!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn3Ge7jBkwTA"
      },
      "source": [
        "def loss_function_deriv(X, W, y_true):\n",
        "    # TODO - код вектора производных (градиента)\n",
        "    return loss_deriv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuoGXeD0XhoT"
      },
      "source": [
        "# TEST\n",
        "X = np.array([\n",
        "    [1, 3],\n",
        "    [4, 2],\n",
        "])\n",
        "W = np.array([1, 1, 2])\n",
        "y_true = np.array([1, 3])\n",
        "\n",
        "dJ = loss_function_deriv(X, W, y_true)\n",
        "dJ_true = np.array([6.5, 15.5, 16.5])\n",
        "\n",
        "assert np.all(dJ == dJ_true)\n",
        "assert np.all(dJ.shape == dJ_true.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzFsrg-SDTKU"
      },
      "source": [
        "Также, для удобства напишем функцию обучения модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6MEbD0s7Exq"
      },
      "source": [
        "def fit_model(lr, n_iter, X, y):\n",
        "    new_W = np.zeros(X.shape[1]+1)\n",
        "    loss_history = []\n",
        "    print(f'Data shape: {X.shape}')\n",
        "    print(f'Start weights: {new_W}')\n",
        "\n",
        "    for i_iter in range(n_iter):\n",
        "        # TODO - добавьте код обновления весов и вычисления предсказаний\n",
        "     \n",
        "        loss = loss_function(y, y_pred)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "    print(f'Result weights: {new_W}')\n",
        "    return new_W, loss_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4-rVDPPQPVa"
      },
      "source": [
        "new_W, loss_history = fit_model(\n",
        "    lr=0.01,\n",
        "    n_iter=1000,\n",
        "    X=X_data,\n",
        "    y=y_data\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Js8aDx3l4Gp"
      },
      "source": [
        "# TEST\n",
        "assert np.all(np.isclose(new_W, np.array([0.76555, 0.77251])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIRCSrJXhUR4"
      },
      "source": [
        "## Задание\n",
        "Изучите, как влияет значение `lr` на скорость сходимости - проверьте значения: [1, 0.1, 0.01, 0.001, 1e-4], отобразите графики функции потерь и предсказания модели, сделайте выводы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-DeaIfl94zR"
      },
      "source": [
        "# TODO - обучите модели при разных коэффициентах обучения \n",
        "#           и отобразите графики истории"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYxNSbfBjO5v"
      },
      "source": [
        "# Полиномиальная регрессия"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucFxWHyKjN7I"
      },
      "source": [
        "n_points = 50\n",
        "\n",
        "real_W = [1, 4, -2]\n",
        "X_data = 3*np.sort(np.random.rand(n_points, 1), axis=0)\n",
        "noize = 2*(np.random.rand(n_points, 1)-0.5)\n",
        "y_data_true = real_W[0] + real_W[1]*X_data + real_W[2]*X_data**2\n",
        "y_data_noized = y_data_true + noize\n",
        "y_data = y_data_noized[:, 0]\n",
        "\n",
        "X_render = np.linspace(X_data.min(), X_data.max(), 100)\n",
        "y_render = real_W[0] + real_W[1]*X_render + real_W[2]*X_render**2\n",
        "\n",
        "plt.scatter(X_data, y_data_noized, label='Данные')\n",
        "plt.plot(X_render, y_render, 'k--', label='Истинная зависимость')\n",
        "plt.ylabel('$Y$')\n",
        "plt.xlabel('$X$')\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mDOe0cW96Sa"
      },
      "source": [
        "Полиномиальная зависимость имеет вид\n",
        "\n",
        "$$\n",
        "y = w_0+w_1*x+w_2*x^2\n",
        "$$\n",
        "\n",
        "Такое представление можно выразить через линейную зависимость, если принять, что квадрат признака - это новый признак, то есть:\n",
        "$$\n",
        "\\begin{align}\n",
        "x_1 = x \\\\\n",
        "x_2 = x^2 \n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "При этом набор данных и веса с полиномиальными признаками будут иметь вид в матричном представлении:\n",
        "$$\n",
        "X = \n",
        "\\begin{bmatrix}\n",
        "1 & x^{(1)}_1 & x^{(1)}_2 \\\\\n",
        "1 & x^{(2)}_1 & x^{(2)}_2 \\\\\n",
        "\\vdots \\\\\n",
        "1 & x^{(n)}_1 & x^{(n)}_2 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W = \n",
        "\\begin{bmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "w_2 \\\\\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5qfoaJkBkNC"
      },
      "source": [
        "При этом, обратите внимание, что функция предсказания $h_W(X)$ остается прежней. Это означает, что мы свели вопрос полиномиальной зависимости к решению через линейную регрессию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hiw_Uo7dADYO"
      },
      "source": [
        "Так как в модели уже три веса, то отобразить плоскость функции потерь затруднительно, но мы можем проверить работоспособность наших функций, которые были написаны ранее.\n",
        "\n",
        "Перед этим нам нужно сформировать преобразование, которое будет приводить вектор данных к представлению для линейной регрессии. Называется такой шаг - генерация полиномиальных признаков."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2GjPWDmj5ws"
      },
      "source": [
        "def generate_polynomial_features(X, degree):\n",
        "    # TODO - код генерации полиномиальных признаков\n",
        "    #           из вектора X\n",
        "    return X_poly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-itMDEGhopNO"
      },
      "source": [
        "# TEST\n",
        "X = np.array([\n",
        "    [1],\n",
        "    [2],\n",
        "    [3],\n",
        "])\n",
        "\n",
        "X_poly = generate_polynomial_features(X, degree=2)\n",
        "assert np.all(X_poly == np.array([[1, 1], [2, 4], [3, 9]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DDUrhr0B5u5"
      },
      "source": [
        "X_poly = generate_polynomial_features(X_data, degree=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZXV1KkyCJ1b"
      },
      "source": [
        "# Задаем примерные начальные веса\n",
        "selected_W = np.array([1.1, 1.2, 1.3])\n",
        "X = generate_polynomial_features(X_data, degree=2)\n",
        "y_true = y_data\n",
        "\n",
        "# Проверяем предикт\n",
        "y_pred = predict(X, selected_W)\n",
        "\n",
        "y_pred.shape, y_pred.shape[0] == X_data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii1ZfsnxE7kp"
      },
      "source": [
        "# Проверяем показатель метрики\n",
        "rmse_value = rmse_score(y_true, y_pred)\n",
        "\n",
        "rmse_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUr0D3XUFTH9"
      },
      "source": [
        "Для отображения полиномиальной зависимости нужно немного обновить функцию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAvhe2JPCan4"
      },
      "source": [
        "plot_model(X_data, y_pred, y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr7-U11KETWk"
      },
      "source": [
        "new_W, loss_history = fit_model(\n",
        "    lr=0.01,\n",
        "    n_iter=2000,\n",
        "    X=X,\n",
        "    y=y_data\n",
        ")\n",
        "\n",
        "show_loss(loss_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oENHTzB0HNJg"
      },
      "source": [
        "y_pred = predict(X, new_W)\n",
        "plot_model(X_data, y_pred, y_true)\n",
        "rmse_score(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hmBjvc7WOEP"
      },
      "source": [
        "# Underfit (high bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCzIXSCiWUtH"
      },
      "source": [
        "Эффект Underfit - эффект, когда сложность модели меньше, чем требуемая для описания данных. Самый простой пример, выбор модели первого порядка для описания зависимости второго порядка:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGPG2sg2HQPl"
      },
      "source": [
        "DEGREE=1\n",
        "X = generate_polynomial_features(X_data, degree=DEGREE)\n",
        "y_true = y_data\n",
        "\n",
        "new_W, loss_history = fit_model(\n",
        "    lr=0.01,\n",
        "    n_iter=2000,\n",
        "    X=X,\n",
        "    y=y_data\n",
        ")\n",
        "\n",
        "y_pred = predict(X, new_W)\n",
        "plot_model(X_data, y_pred, y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mreZp04GaRlj"
      },
      "source": [
        "show_loss(loss_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwYg3GtK1ruO"
      },
      "source": [
        "# R2 метрика (коэффициент детерминации)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcJCIIIRYzoV"
      },
      "source": [
        "Как видно, модель \"плохо\" описывает данные, при этом градиентный спуск нашел веса для условия минимума функции потерь. Функции $RMSE$ и $MAE$ хорошо использовать для относительных сравнений работы моделей, так как они не имеют верхнего предела. Для абсолютной оценки удобно использовать метрику $R^2$ (r-squared) - коэффициент детерминации.\n",
        "\n",
        "Вычисляется оценка с помощью двух составляющих:\n",
        "- Сумма квадратов отклонений данных (total sum of squares)\n",
        "$$\n",
        "SS_{tot}=\\sum_{i}(y^{(i)}-\\bar{y})^2\n",
        "$$\n",
        "\n",
        "- Сумма отклонений предсказаний (sum of squares of residuals)\n",
        "$$\n",
        "SS_{res}=\\sum_{i}(y^{(i)}-h_W^{(i)}(x^{(i)}))^2\n",
        "$$\n",
        "\n",
        "где $\\bar{y}=\\frac{1}{n}\\sum_{i}y^{(i)}$\n",
        "\n",
        "Сама оценка рассчитывается следующим образом:\n",
        "$$\n",
        "R^2=1-\\frac{SS_{res}}{SS_{tot}}\n",
        "$$\n",
        "\n",
        "Особенностью показателя является то, что он имеет верхний предел 1.0, который достигается в случае, если отклонения предсказаний всегда равны нулю. Нижний предел не ограничивается. Суть коэффициента детерминации в том, что в качестве худшего случая предсказания принимается линия, равная среднему значению $y$. При таком варианте коэффициент равен нулю. Если модель предсказывает лучше, чем худший случай, то значение будет варьироваться от 0 до 1. Если модель описывает данные хуже, чем \"всегда среднее\", то такой кейс считается неприемлимым."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI7PRIIrXYEf"
      },
      "source": [
        "def r2_score(y_true, y_pred):\n",
        "    # TODO - код расчета R-squared\n",
        "    return r2_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26pkiV3Cp6v5"
      },
      "source": [
        "# TEST\n",
        "y_true = np.array([0.5, 0.1, -0.4, 1.2])\n",
        "y_pred = np.array([0.2, 0.0, -0.1, 3.1])\n",
        "\n",
        "r2_value = r2_score(y_true, y_pred)\n",
        "\n",
        "assert np.isclose(r2_value, -1.773722)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvSkX3iYb__b"
      },
      "source": [
        "y_true = y_data\n",
        "y_pred = predict(X, new_W)\n",
        "r2_value = r2_score(y_true, y_pred)\n",
        "\n",
        "r2_value, r2_value > 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I_6UNJdgds1"
      },
      "source": [
        "В данном случае мы видим, что модель предсказывает лучше, чем просто среднее значение вектора истинного значения, что уже хорошо. Но при этом показатель недостаточно высок, чтобы сказать, что модель \"хорошо\" описывает данные - случай underfit. Это видно как на графике, так и выведенно численно ($R^2$). Для примера сравните с показателем при обучении модели второго порядка."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_nPZXZjhIEy"
      },
      "source": [
        "## Задание\n",
        "Произведите обучение модели второго порядка и оцените коэффициент детерминации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AXKn-BodLlP"
      },
      "source": [
        "# TODO - обучите модель при признаках второго порядка и оцените R2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAMNtRPAhUaJ"
      },
      "source": [
        "# Overfit (high variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwIB8NZRj8oW"
      },
      "source": [
        "Случай overfit является обратным к underfit, когда модель является слишком сложной (комплексной), нежели требуется для описания данных. Для разбора такого случая воспользуемся моделью десятого порядка для данных, которые имеют зависимость второго порядка. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdRGJoJz2Zkg"
      },
      "source": [
        "> Пример сгенерирован с использованием `numpy.linalg.lstsq` - метод наименьших квадратов, который применим к линейной регрессии. Это аналитический метод, который находит наилучшее решение для линейных методов. Связано это с тем, что при большом количестве признаков пространство ошибок становится крайне неравномерным, а из-за итеративности метода градиентного спуска он останавливается в локальном минимуме."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYumOVjo18aR"
      },
      "source": [
        "DEGREE=25\n",
        "X = generate_polynomial_features(X_data, degree=DEGREE)\n",
        "y = y_data\n",
        "\n",
        "def fit_model_least_square(X, y):\n",
        "    coef_, _residues, rank_, singular_ = np.linalg.lstsq(X, y)\n",
        "    coef_ = np.insert(coef_, 0, 0)\n",
        "    new_W = np.array(coef_)\n",
        "\n",
        "    return new_W\n",
        "\n",
        "new_W = fit_model_least_square(X, y)\n",
        "\n",
        "y_pred = predict(X, new_W)\n",
        "plot_model(X_data, y_pred, y_data)\n",
        "rmse_score(y_true, y_pred), r2_score(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swnuysYNVbFe"
      },
      "source": [
        "Один из простых методов для определения overfit является подход **hold-out**, при котором происходит разделение всей выборки на обучающую и тестовую выборки в соотношении 70/30 или 80/20 (чем больше данных в целом, тем меньше может быть процент тестовой выборки). Таким образом модель учится на обучающей выборке, а тестовая используется лишь для оценки показателей метрик."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaG-q70p31wF"
      },
      "source": [
        "DATA_COUNT=X_data.shape[0]\n",
        "# np.random.permutation() - функция перемешивания данных \n",
        "#   в переданном массиве\n",
        "data_indexes=np.random.permutation(range(DATA_COUNT))\n",
        "\n",
        "# 70% данных уходит на обучение\n",
        "TRAIN_COUNT=int(DATA_COUNT*0.7)\n",
        "train_indexes=data_indexes[:TRAIN_COUNT]\n",
        "test_indexes=data_indexes[TRAIN_COUNT:]\n",
        "\n",
        "X_train = X_data[train_indexes]\n",
        "y_train = y_data[train_indexes]\n",
        "X_test = X_data[test_indexes]\n",
        "y_test = y_data[test_indexes]\n",
        "\n",
        "X_train = generate_polynomial_features(X_train, degree=DEGREE)\n",
        "X_test = generate_polynomial_features(X_test, degree=DEGREE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCmZhhm1jQ0B"
      },
      "source": [
        "new_W = fit_model_least_square(X_train, y_train)\n",
        "\n",
        "y_pred = predict(X_test, new_W)\n",
        "test_mse = rmse_score(y_test, y_pred)\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "y_pred = predict(X_train, new_W)\n",
        "train_mse = rmse_score(y_train, y_pred)\n",
        "train_r2 = r2_score(y_train, y_pred)\n",
        "\n",
        "print(f'Train MSE: {train_mse} / R2: {train_r2}')\n",
        "print(f'Test MSE: {test_mse} / R2: {test_r2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUNdoDsDV5jA"
      },
      "source": [
        "Как видим, явным признаком ситуации overfit является сильная разница в показаниях метрик обучащей и тестовой выборке."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy30aAZcakek"
      },
      "source": [
        "X_min = X_data[:,0].min()\n",
        "X_max = X_data[:,0].max()\n",
        "\n",
        "X_render = np.linspace(X_min, X_max, 100).reshape(-1, 1)\n",
        "# Добавим генерацию полиномиальных признаков\n",
        "X_render = generate_polynomial_features(X_render, DEGREE)\n",
        "y_render = predict(X_render, new_W)\n",
        "\n",
        "plt.scatter(X_train[:,0], y_train, color='blue', label='Данные (обучение)')\n",
        "plt.scatter(X_test[:,0], y_test, color='red', label='Данные (тест)')\n",
        "plt.plot(X_render[:,0], y_render, 'k--', label='Предсказание модели')\n",
        "plt.ylabel('$Y$')\n",
        "plt.xlabel('$X$')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6ciwBpJAkad"
      },
      "source": [
        "По сути, природой такого эффекта является то, что вместо отражения общих зависимостей, модель старается как можно точнее отработать на выборке, на которой обучается. Таким образом, модель отдаляется от работы с зависимостью и сильнее реагирует на шумы в данных.\n",
        "\n",
        "> Эффект overfit еще сравнивают с \"запоминанием\" данных моделью. Это связано с тем, что при большой комплексности модели она может запомнить конкретные точки (на примере полиномов - высокий порядок позволяет сильнее изгибаться), что ведет к невозможности адекватно предсказывать на новых данных.\n",
        "\n",
        "Как видно на картинке, увеличение комплексности модели ведет к увеличению Variance и уменьшению Bias. Так, слишком комплексная модель начинает заниматься запоминанием, а не построением общей зависимости. При недостаточно комплексности высок Bias, что ведет к невозможности описать зависимости.Наилучший вариант, когда модель близка по комплексности к данным, на которых обучается.\n",
        "\n",
        "![title](https://miro.medium.com/max/492/1*kADA5Q4al9DRLoXck6_6Xw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awqOmPRrQkXZ"
      },
      "source": [
        "Таким образом, эффекты Underfit/Overfit достаточно распространены в работе с моделями. При этом если Underfit можно увидеть уже на моменте оценки работы модели, то для избежания эффекта Overfit можно перечислить некоторые методы работы помимо Holt-Out (Train/Test):\n",
        "- Увеличение количества данных;\n",
        "- Методы **регуляризации** (Ридж, Лассо);\n",
        "- Кросс-валидация;\n",
        "- Уменьшение количества признаков;\n",
        "- Ансамблирование;\n",
        "- и др. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oTv0yGO22bA"
      },
      "source": [
        "# Стандартизация/нормализация значений признаков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfoYVqrfGOJx"
      },
      "source": [
        "Теперь давайте возьмем наши данные, которые имеют полиномиальную зависимость и попробуем описать ее полиномом 7-го порядка:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B87WW_fatJj0"
      },
      "source": [
        "DEGREE=7\n",
        "X_poly = generate_polynomial_features(X_data, degree=DEGREE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiTitV0IvEbV"
      },
      "source": [
        "X = X_poly\n",
        "y_true = y_data\n",
        "\n",
        "new_W, loss_history = fit_model(\n",
        "    lr=0.01,\n",
        "    n_iter=2000,\n",
        "    X=X,\n",
        "    y=y_true\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YnqF4ku4HCR"
      },
      "source": [
        "Что-то странное произошло - все веса стали `nan` (Not A Number). Для того, чтобы разобраться с проблемой, в первую очередь стоить посмотреть на значения вычисленных производных (величину градиента)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC_8CM1vv1F8"
      },
      "source": [
        "start_W = np.zeros(DEGREE+1)\n",
        "dJ = loss_function_deriv(X, start_W, y_true)\n",
        "print(dJ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2LY5QVi4VR5"
      },
      "source": [
        "Как видно, значения производных первых признаков на порядок отличается от значений производных последних признаков. Большие значения производных (большая величина градиента) ведет к огромному шагу в пространстве признаков, что приводит к проблеме **расходящегося градиента**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eb2o4rlyorI"
      },
      "source": [
        "![КАРТИНКА С РАСХОДЯЩИМСЯ ГРАДИЕНТОМ](https://miro.medium.com/max/700/1*hGhRddOUV8h0pdQek8T35A.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be6Y0R_-3m7j"
      },
      "source": [
        "Для решения проблемы расходящегося градиента можно уменьшить коэффициент обучения, тем самым уменьшив шаг перемещения в пространстве весов - это повышает стабильность алгоритма."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb9c9xtFvs3N"
      },
      "source": [
        "new_W, loss_history = fit_model(\n",
        "    lr=1e-6,\n",
        "    n_iter=2000,\n",
        "    X=X,\n",
        "    y=y_true\n",
        ")\n",
        "y_pred = predict(X, new_W)\n",
        "plot_model(X_data, y_pred, y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oiU18CZtMGI"
      },
      "source": [
        "По графику обученная модель плохо предсказывает данные.\n",
        "\n",
        "Вспомним значения градиентов, у признаков ближе к нулевому индексу значение более тысячи по модулю, хотя производная последнего элемента меньше единицы по модулю. Это сказывается на процессе обучения: при большом коэффициенте обучения градиент расходится, при малом - веса обновляются очень долго и при этом только те веса, у которых градиент достаточно большой, чтобы умножение на очень малый коэффициент обучения дало хоть какое-то обновление.\n",
        "\n",
        "> Именно эта проблема еще называется проблемой зависания в \"плато\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrvTpVL1uQkm"
      },
      "source": [
        "Таким образом, различия в пределах распределения признаков в данных может очень сильно сказаться на процессе обучения, так как признаки с большими значениями сильнее реагируют на изменения по сравнению с признаками, которые имеют малые значения, при том, что коээфициент обучения один."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2D8FK2yv4Ao"
      },
      "source": [
        "Для решения данной проблемы выполняется процесс стандартизации. Он заключается в том, чтобы привести все признаки к единому диапазону за счет добавления шага предобработки данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxpruVBQwGX3"
      },
      "source": [
        "Наиболее распространенным процессом стандартизации является приведение распределения к распределению с нулевым средним (центрования) и единичным стандартным отклонением (машстабирование).\n",
        "\n",
        "> Связано это с тем, что огромные массивы случайных данных вероятнее всего имеют нормальное распределение. На деле признаки не всегда имеют данный характер распределения, но тем не менее оно остается желанным, так как с ним проще работать. Как известно, нормальное распределение характеризуется средним и стандартным отклонением, именно поэтому процесс стандартизации использует эти два шага."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g_J9lEnxYRw"
      },
      "source": [
        "Для вычисления средних значений и стандартных отклонений для каждого признака воспользуеся методами `mean()` и `std()` из пакета `numpy` по оси записей (аргумент `axis`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXi37u93xznP"
      },
      "source": [
        "Принцип нормализации основывается на формуле:\n",
        "\n",
        "$$\n",
        "X_{standardized}=\\frac{X-X_{mean}}{X_{std}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69BTelQXxsc2"
      },
      "source": [
        "def standardize_features_scale(X):\n",
        "    # TODO - код для стандартизации\n",
        "    return X_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "totmF7qh2fXR"
      },
      "source": [
        "# TEST\n",
        "X = np.arange(2, 10).reshape(4, 2)\n",
        "\n",
        "X_scaled = standardize_features_scale(X)\n",
        "\n",
        "# Cтандартизированные данные должны иметь нулевое среднее и единичное стандартное отклонение\n",
        "assert np.all(np.isclose(X_scaled.mean(axis=0), 0))\n",
        "assert np.all(np.isclose(X_scaled.std(axis=0), 1))\n",
        "\n",
        "assert np.all(\n",
        "    np.isclose(\n",
        "        X_scaled, \n",
        "        np.array([[-1.34164079, -1.34164079],\n",
        "                  [-0.4472136 , -0.4472136 ],\n",
        "                  [ 0.4472136 ,  0.4472136 ],\n",
        "                  [ 1.34164079,  1.34164079]])\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkLrC7Vly86W"
      },
      "source": [
        "Посмотрим на значения производных при использовании стандартизованных признаков:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-TLnvUEymxf"
      },
      "source": [
        "X_poly_scaled = standardize_features_scale(X_poly)\n",
        "y_true = y_data\n",
        "start_W = np.zeros(DEGREE+1)\n",
        "\n",
        "dJ = loss_function_deriv(X_poly_scaled, start_W, y_true)\n",
        "print(dJ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbIylGA0zULq"
      },
      "source": [
        "При корректной стандартизации вычисление производных должно показать значения в одном порядковом диапазоне (единицы). Это означает, что во время обучения некоторые признаки не будут так сильно влиять на изменение весов. Все веса будут обновляться равномерно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1frrkSI5HAM"
      },
      "source": [
        "new_W, loss_history = fit_model(\n",
        "    lr=0.01,\n",
        "    n_iter=2000,\n",
        "    X=X_poly_scaled,\n",
        "    y=y_true\n",
        ")\n",
        "y_pred = predict(X_poly_scaled, new_W)\n",
        "plot_model(X_data, y_pred, y_data)\n",
        "r2_score(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVYrWl0qzl7f"
      },
      "source": [
        "> Не забывайте важную вещь! При введении шага стандартизации все данные, поступаемые для получения предсказаний, должны быть стандартизованы. То есть, процесс обычно происходит так: обучающие данные стандартизуются, вычисленные средние и стд. отклонения для признаков сохраняются и далее новые данные стандартизуются уже по сохраненным параметрам."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TBCKSt40Kap"
      },
      "source": [
        "Как видно, обучение со стандартизованными значениями признаков проходит без расходящегося градиента даже при исходном значении коэффициента обучения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r5tCQLSrLLl"
      },
      "source": [
        "## Задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFYDdcAmrPKN"
      },
      "source": [
        "Реализуйте и проверьте работоспособность стандартизации минмакс:\n",
        "$$\n",
        "X_{standardized}=\\frac{X-X_{min}}{X_{max}-X_{min}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G01aC4QirMlH"
      },
      "source": [
        "def minmax_feature_scale(X):\n",
        "    # TODO - код масштабирования по принципу minmax\n",
        "    return X_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O3orOdS5I8b"
      },
      "source": [
        "# TEST\n",
        "X = np.arange(2, 10).reshape(4, 2)\n",
        "\n",
        "X_scaled = minmax_feature_scale(X)\n",
        "\n",
        "assert np.all(\n",
        "    np.isclose(\n",
        "        X_scaled, \n",
        "        np.array([[0.        , 0.        ],\n",
        "                  [0.33333333, 0.33333333],\n",
        "                  [0.66666667, 0.66666667],\n",
        "                  [1.        , 1.        ]])\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4um8Ygn5JYy"
      },
      "source": [
        "# TODO - обучите модель и проверьте предсказания"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R75JT_Ou5qgM"
      },
      "source": [
        "# Стандартизация для линейной регрессии (важность признаков)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhK939u05t2x"
      },
      "source": [
        "Стандартизация имеет не только влияние на процесс обучения, но и важна для модели линейной регрессии и ее применения. Как помнится, модель линейной регрессии представляет собой сумму значений признаков, умноженных на веса. Для примера возьмем модель с двумя признаками и нулевым смещением ($w_0=0$).\n",
        "\n",
        "$$\n",
        "y = 2*x_1 + 20*x_2\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrdyMqT37hqV"
      },
      "source": [
        "Идеологически назначение весов в линейной регрессии в том, чтобы показать, какой вклад составляет каждый признак в предсказание. Без знания и описания данных, на которых обучена модель, можно лишь сказать, что признак $x_2$ имеет в 10 раз больший вклад (важность) в результат предсказания, нежели $x_1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjIpDNwD8fNv"
      },
      "source": [
        "Теперь приложим больше подробностей:\n",
        "- Зависимая переменная $y$ - количество баллов на экзамене $[0; 100]$;\n",
        "- Признак $x_1$ - количество выполненных заданий $[0; 40]$;\n",
        "- Признак $x_2$ - отношение количества посещений к количеству прошедших занятий $[0; 1]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUi-RhOqGCBu"
      },
      "source": [
        "Попробуем представить пример таких данных:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poNuzz7wEFUh"
      },
      "source": [
        "W = [2, 20]\n",
        "\n",
        "X = np.random.uniform(low=0, high=[40, 1], size=(15, 2))\n",
        "X[:, 0] = X[:, 0].astype(int)\n",
        "y_true = np.clip(X.dot(W)+np.random.randn(15)*3, 0, 100)\n",
        "\n",
        "df = pd.DataFrame(X[:,:2], columns=['tasks_completed', 'class_attended_rate'])\n",
        "df['grade'] = y_true\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKZDc7G6EDQp"
      },
      "source": [
        "Что можно отметить: те студенты, которые почти не ходили на занятия, но выполнили большое количество заданий получили высокую оценку. Обратная ситуация, были на всех занятиях, но при этом выполнили мало задач - никто не получил хорошей оценки.\n",
        "\n",
        "При этом, показатель R2 говорит об отличном соответствии модели данным. Так в чем проблема?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R71K_i9yHv3D"
      },
      "source": [
        "y_pred = X.dot(W)\n",
        "r2_score(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-G0R-A4H1pI"
      },
      "source": [
        "Проблема заключается в том, что даже при супер-точных предсказаниях, модель не отражает реальной зависимости. Она отражает зависимости в данных, которые имеют разные распределения (или как минимум диапазоны), что приводит к ошибочному выводу о том, что посещение занятий в 10 раз важнее, чем количество задач на экзамене.\n",
        "\n",
        "Веса признаков отражают важности этих признаков, поэтому при построении модели нужно всегда учитывать распределение признаков."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHWykhAwI9Gf"
      },
      "source": [
        "X_scaled = standardize_features_scale(X)\n",
        "\n",
        "new_W, _ = fit_model(\n",
        "    lr=0.01,\n",
        "    n_iter=3000,\n",
        "    X=X_scaled,\n",
        "    y=y_true\n",
        ")\n",
        "\n",
        "y_pred = predict(X_scaled, new_W)\n",
        "new_W, r2_score(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI0BRyFtUgvJ"
      },
      "source": [
        "Как видно, модель, обученная после стандартизации отражает другое отношение весов при признаках $x_1$ (21.5) и $x_2$ (5.9), что больше коррелирует с реальной зависимостью."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M_OhH2b9htJ"
      },
      "source": [
        "> Именно по этой причине очень важно всегда стараться разобраться в данных, признаках и зависимостях, чтобы интерпретировать данные правильно. Плохое понимание данных может составить дополнительные сложности как в обучении, так и в разборе модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9b1-SAgtXEQ"
      },
      "source": [
        "# Выводы - задание\n",
        "\n",
        "Напишите выводы по итогам изучения материала. Вам может помочь следующий список вопросов, но лучше еще и добавить свои умозаключения!\n",
        "\n",
        "- Почему лучше использовать большое количество данных? \n",
        "- Что такое обучение модели и зачем это нужно? \n",
        "- Что лучше визуальная оценка работы системы или численная? Почему? \n",
        "- В чём заключается задача оптимизации? Почему оптимизация нужна? \n",
        "- Что за функция потерь и что она теряет? Может быть вообще эта функция только усложняет жизнь? \n",
        "- Что такое градиентный спуск и градиентный подъём? Существует ли градиентное плато?\n",
        "- Какие эффекты могут произойти при работе с данными? Как их вовремя распознать? А нужно ли их распознавать, может и так сойдёт? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCP0fS8I39JU"
      },
      "source": [
        "# Вопросики!\n",
        "\n",
        "Ответив на них вы точно можете считать себя джедаем этой темы!\n",
        "\n",
        "- Что такое зависимость в данных?\n",
        "- Как можно описать линейный характер распределения данных? \n",
        "- Что такое задача определения регрессии? \n",
        "- Что такое модель в машинном обучении? \n",
        "- Что такое обучение модели и зачем это нужно? \n",
        "- Опишите процесс работы с данными и моделью, начиная от получения данных и заканчивая получением предсказания. \n",
        "- Почему вес важен в машинном обучении?  \n",
        "- Какие метрики работают по правилу “меньше значение метрики - лучше модель работает”?\n",
        "- В каком случае лучше применять метрику квадратичного отклонения? \n",
        "- В чём заключается задача оптимизации?\n",
        "- Что такое “определение градиента”? \n",
        "- Опишите что происходит при градиентном расхождении? \n",
        "- Опишите эффект underfit? Приведите пример \n",
        "- В чём особенности метрики R2? \n",
        "- Приведите пример эффекта overfit?\n"
      ]
    }
  ]
}